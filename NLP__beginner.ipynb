{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP _beginner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhFeNu5hy0qltnFnpfeS86",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bishab/Data-science-best-resources/blob/master/NLP__beginner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UfnKqwFHAn5"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMuqkCbEHZXG"
      },
      "source": [
        "train_x=['I love the book', 'this is a great book','the fit is great', 'i love the sheets']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EslrjNkNHjQk"
      },
      "source": [
        "vectorizer=CountVectorizer()\n",
        "vectors=vectorizer.fit_transform(train_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xEhdslvHzPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cb34ff9-6e1a-4746-f1aa-b736bf506d4f"
      },
      "source": [
        "print(vectorizer.get_feature_names()) #gives the list of feature_names\n",
        "print(vectors.toarray()) #prints one hot vectors as arrays"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['book', 'fit', 'great', 'is', 'love', 'sheets', 'the', 'this']\n",
            "[[1 0 0 0 1 0 1 0]\n",
            " [1 0 1 1 0 0 0 1]\n",
            " [0 1 1 1 0 0 1 0]\n",
            " [0 0 0 0 1 1 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cf1FON6hd0K"
      },
      "source": [
        "#TOKENIZING\n",
        "In tokenizing, we assign a value to our words! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYlhnBtejCag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06a1238a-5b51-4f77-82d0-874d113a97fd"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentence=['i love my car','i love my bike']\n",
        "\n",
        "tokenizer=Tokenizer(num_words=100)\n",
        "tokenizer.fit_on_texts(sentence)\n",
        "word_index=tokenizer.word_index\n",
        "print(word_index)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'i': 1, 'love': 2, 'my': 3, 'car': 4, 'bike': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXImb41yhlno"
      },
      "source": [
        "#SEQUENCING\n",
        "After tokenizing, we have the keywords, but how to know in which sequence the keywords are? So for that, we use sequencing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir9lJCe2gmRE",
        "outputId": "0604de5d-53ce-415d-eeb3-0eb89eedbfdf"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentence=['i love my car','i love my bike']\n",
        "\n",
        "tokenizer=Tokenizer(num_words=100,oov_token='<OOV>')  # Here, OOV is used because while using test data, if some of the words in test data is unknown to the model, it will atleast put a place for that and does not ignore it.\n",
        "\n",
        "tokenizer.fit_on_texts(sentence)\n",
        "word_index=tokenizer.word_index\n",
        "print(word_index)\n",
        "\n",
        "sequence=tokenizer.texts_to_sequences(sentence)\n",
        "print(sequence)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'i': 2, 'love': 3, 'my': 4, 'car': 5, 'bike': 6}\n",
            "[[2, 3, 4, 5], [2, 3, 4, 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxf0qi7eL84W"
      },
      "source": [
        "Now, although the data is converted into its corresponding keywords, are all they of same size? Of course not. They may be same size here, but in thousand of sentences, we cannot find all the sentences to be of same size. That is where we need to convert them all into the same size. That is what we called Padding, right?\n",
        "\n",
        "So in NLP, we do padding using pad_sequences below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjoN9wgQ3xnZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36631383-9eb8-401f-d2dd-8acebc6121a4"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "#Now just pass the sequence to it! \n",
        "padded=pad_sequences(sequence)\n",
        "print(padded)\n",
        "#We can see that two sentences are padded accordingly. Nice, isn't it?\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2 3 4 5]\n",
            " [2 3 4 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akrBu2sFMXOV"
      },
      "source": [
        "new copy made"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}